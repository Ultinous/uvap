/*
Structures for Kafka streams containing Ultinous video analysis results.
Copyright (C) 2014,2018 Ultinous Inc.
*/

////////////////////////////////////////////////////////////////////////////////////////////////////////
//
// This proto describes the Kafka messages created as a result of running video analysis components.
// They are all outputs of one or more components. Some of them are also input to complex components.
//
// Each record below has a time stamp and a key, in addition to the payload described by the proto message.
// The time and key are documented above each record.
//
// Some of the records below are linked by sharing the same key in order to be able to identify one with another.
// For example, AgeRecord has the same key as ObjectDetectionRecord so that each age can be assigned to
// the correseponding person when multiple persons are detected in a single frame.
//
////////////////////////////////////////////////////////////////////////////////////////////////////////

syntax = "proto3";

import "ultinous/proto/common/skeleton.proto";

package ultinous.proto.kafka;

option java_package = "com.ultinous.proto.kafka";
option java_multiple_files = true;

import "ultinous/proto/common/kafka_common.proto";

/** Output of MGR Object Detection.

  One instance of this record is generated for each detected head/face on each frame.

  For each analyzed video frame, a series of detection records are created, corresponding to the number of
  people deteceted in that frame. The detection records belonging to a single video frame are indexed sequentially
  thus these indices are only unique within a single frame. Therefore a combined key is generated from the
  the timestamp of the source video frame and the detection index to make it unique for the entire video stream.

  The end_of_frame field indicates that no more records for the given input video frame will be inserted into the stream.
  When this flag is true, all other fields of the record are invalid.

  time: timestamp of the input video frame
  key: time + "_" + sequential index within frame
*/
message ObjectDetectionRecord
{
  ObjectType type = 1; ///< Type of detected object.
  Rect bounding_box = 2; ///< Rectangular box containing the head/face.
  float detection_confidence = 3; ///< Confidence of the algorithm about a head being detected. Range: [0..1]
  bool end_of_frame = 4; ///< When true, no more results will be returned for this frame; all other fields of the record are invalid.
}

/** 3D rotation coordinates for head pose.
  See e.g. https://howthingsfly.si.edu/flight-dynamics/roll-pitch-and-yaw
  Coordinates can be positive or negative as well.
  (0, 0, 0) degrees means the head directly facing the camera.
  unit: degrees
*/
message Orientation3D {
  float yaw = 1; ///< Yaw.
  float roll = 2; ///< Roll.
  float pitch = 3; ///< Pitch.
}

/** Output of MGR Head Pose Detection.

  One instance of this record is generated for a detection if its head pose can be determined.

  time: timestamp of the input video frame
  key: same as the key of the corresponding ObjectDetectionRecord
*/
message HeadPose3DRecord
{
  Orientation3D pose = 1; ///< Rotation coordinates.
  bool end_of_frame = 2; ///< When true, no more results will be returned for this frame; all other fields of the record are invalid.
}

/** Feature vector
*/
message FeatureVector {
  enum FeatureType
  {
    PERSON_FACE = 0; ///< Face feature calculation result type
    PERSON_FULL_BODY = 1; ///< Full-body feature calculation result type
  }
  string model_id = 1; ///< Unique id of the model (neural network) that generated the vector.
  repeated float feature = 2; ///< Model specific internal feature representation.
  FeatureType type = 3; ///< Type of features represented by the feature vector.
}

/** Output of MGR Feature Vector Detection.
  Feature vectors are an internal representation of the characteristics of a specific person's face or full body.
  This record type is only to be used by other Ultinous software components, e.g. for face recognition.

  One instance of this record is generated for a detection if its feature vector can be determined.

  time: timestamp of the input video frame
  key: same as the key of the corresponding ObjectDetectionRecord
*/
message FeatureVectorRecord
{
  FeatureVector features = 1; ///< Internal representation of the detected face or full body.
  bool end_of_frame = 2; ///< When true, no more results will be returned for this frame; all other fields of the record are invalid.
}

/** Output of MGR Gender Detection.

  One instance of this record is generated for a detection if its gender can be determined.

  time: timestamp of the input video frame
  key: same as the key of the corresponding ObjectDetectionRecord
*/
message GenderRecord
{
  enum Gender {
    MALE = 0;
    FEMALE = 1;
  }

  Gender gender = 1; ///< Gender of detected person.
  float confidence = 2; ///< Confidence of the algorithm about the gender decision. Range: [0..1]
  bool end_of_frame = 3; ///< When true, no more results will be returned for this frame; all other fields of the record are invalid.
}

/** Output of MGR Age Detection.

  One instance of this record is generated for a detection if its age can be determined.

  time: timestamp of the input video frame
  key: same as the key of the corresponding ObjectDetectionRecord
*/
message AgeRecord
{
  uint32 age = 1; ///< Age of detected person. Unit: years
  float confidence = 2; ///< Confidence of the algorithm about the age decision. Range: [0..1]
  bool end_of_frame = 3; ///< When true, no more results will be returned for this frame; all other fields of the record are invalid.
}

/** Output of MGR Skeleton Detection.

  One instance of this record is generated for each detected person on each frame.
  A skeleton is a set of points, labelled with a body part label.
  Points are not connected, a possible way of connecting them is described in skeleton.proto.

  time: timestamp of the input video frame
  key: time + "_" + sequential index within frame
*/
message SkeletonRecord
{
  /** A single point in the skeleton.
  */
  message SkeletonPoint
  {
    float x = 1; ///< Horizontal coordinate in pixels, sub-pixel precision
    float y = 2; ///< Vertical coordinate in pixels, sub-pixel precision
    common.SkeletonPointType type = 3; ///< Type corresponding to a specific body part, see skeleton.proto
    float confidence = 4; ///< Point detection confidence. Range: [0..1]
  }

  repeated SkeletonPoint points = 1; ///< Each point has a different type, i.e. each body part occurs either once or not at all.
  bool end_of_frame = 2; ///< When true, no more results will be returned for this frame; all other fields of the record are invalid.
}

/** Output of MGR containing frame information.

  One instance of this record is generated for each input video frame.

  time: timestamp of the input video frame
  key: empty
*/
message FrameInfoRecord
{
  uint32 columns = 1; ///< Number of pixels in the horizontal dimension.
  uint32 rows = 2; ///< Number of pixels in the vertical dimension.
}

/** Output of kafka_tracker.

  One instance of this record is generated for each new point on each track.
  New points can be detected or predicted. In either way, a new point is created for each new frame, until the track ends.

  time: time of the frame
  key: trackStartTime_trackId
    trackId is a serial number starting from 0 when the application starts.
    Note that the key is the same for all TrackChangeRecords of a single track.
*/
message TrackChangeRecord
{
  bool end_of_track = 1; ///< When true, no more results will be returned for this track; all other fields of the record are invalid.
  string detection_key = 2; ///< Empty for predicted points, otherwise same as the key of the corresponding ObjectDetectionRecord.

  Point point = 3; ///< Centroid of the detection or predicted point.
}

/** Identifier structure for a PassEvent
*/
message PassEventId
{
  string track_key = 1; ///< Key of the TrackChangeRecord message corresponding to the track crossing the line in this event.
  uint32 serial = 2; ///< Serial number of the current passage within the same track (starts from 0).
}

/**  Details of a passage event.
  Used by PassDetectionRecord and PassCounterRecord.
*/
message PassEvent
{
  /** Direction of line crossing.
    The pass line itself is considered to have a direction, from its first point to the last one.
    The terms 'left' and 'right' are defined by an observer moving along the direction of the pass line.
    Analogy: left vs. right bank of a river.
  */
  enum CrossDirection
  {
    LR = 0; ///< Left to right crossing
    RL = 1; ///< Right to left crossing
  }

  PassEventId id = 1; ///< Identifier of the current PassEvent.
  string pass_line_id = 2; ///< The 'id' of the PassLine in PassDetConfigRecord being crossed.
  CrossDirection cross_dir = 3; ///< Crossing direction.
  uint32 section_idx = 4; ///< The poly-line segment index that has been crossed, starting from 0.
  Point cross_point = 5; ///< Point of intersection of the track and the pass line.
}

/** Output of kafka_passdet.

  One instance of this record is generated when any of the following events happens:
  E1. Passage event:
    A track crosses a pass line. That is, the previous and current TrackChangeRecord points are
    on opposite sides of the pass line. The last point of the track can be either detected or predicted.
      Key is pass_line_id.
      Time is the time of the frame containing the last point of the track.
  E2. Passage Realization event:
    A prediction based passage event is confirmed by a true detection.
      Key is empty.
      Time is the time of the frame immediately after the passage, i.e. the one containing a prediction for this track.
  E3. End of Track event:
    A track has ended.
      Key is empty.
      Time is the time of the TrackChangeRecord with end_of_track=true.
  E4. Heartbeat event:
    There is input but no message was sent in the last second.
      Key is empty.
      Time is the time of the last input.
*/
message PassDetectionRecord
{
  enum Type
  {
    HEARTBEAT = 0;
    PASS_CANDIDATE = 1;
    PASS_REALIZED = 2;
    END_OF_TRACK= 3;
  }
  message PassCandidate
  {
    PassEvent pass = 1; ///< Details of the passing event
    bool is_extrapolated = 2; ///< True if pass detected with predicted track, false if detected with real track
  }
  message PassRealized
  {
    PassEventId pass_event_ref = 1; ///< Identifier of an earlier PassDetectionRecord with type=PASS_CANDIDATE that is realized.
  }
  message EndOfTrack
  {
    string track_key = 1; ///< TrackChangeRecord key
  }

  Type type = 1; ///< Event type

  oneof details ///< Not set when type = HEARTBEAT
  {
    PassCandidate pass_candidate = 2; ///< Only for type=PASS_CANDIDATE
    PassRealized pass_realized = 3; ///< Only for type=PASS_REALIZED
    EndOfTrack end_of_track = 4; ///< Only for type=END_OF_TRACK
  }
}

/** Output of kafka_roifilter.

  One instance of this record is generated every time an ObjectDetectionRecord passes a filter.
  time: Time of object detection record
  key: Same sas the key of the contained ObjectDetectionRecord
*/
message FilteredObjectDetectionRecord
{
  string filter_id = 1; ///< Identifier of the passing filter
  ObjectDetectionRecord detection = 2; ///< The detection that passed
}

/** Output of kafka_reid.

  One instance of this record is generated when any of the following events happens:
  E1. Reidentification event:
    The feature vector input matches at least one of the stored identities.
    A list of matching identities are returned, together with their score of matching.
  E2. Registration event:
    The feature vector input is either registered as a new identity or updates the feature vector of a stored identity.
  In both cases the returned identities are containing the timestamp of the (first) registration time which can be used for dwell time calculations.

  The input records can be of any record type that contains feature vectors and associated model ids.
  time: Time of the current input record.
  key: Key of the current input record.
*/
message ReidRecord
{
  /** Reference to a specific feature vector input on a specific input stream.
  */
  message SubjectRef
  {
    string stream_id = 1; ///< Same as the 'stream_id' in ReidConfigRecord
    string key = 2; ///< Complex key that is unique for each input event
  }
  /** SubjectRef with a score of matching.
  */
  message ScoredSubjectRef
  {
    SubjectRef subject = 1; ///< See SubjectRef
    float score = 2; ///< Score of matching. Range: [0..1]
  }

  /** Event type.
  */
  enum Type
  {
    REID = 0; ///< Reidentification event
    REG = 1; ///< Registration event
  };

  Type type = 1; ///< Type of event.
  SubjectRef event = 2; ///< New subject generated for the current input record.

  /** A list of stored subject references.
    E1 (reidentification): List of matching identities, in decreasing order of scores
    E2 (registration): Exactly one ScoredSubjectRef (new or updated) that is stored after the registration.
  */
  repeated ScoredSubjectRef reg_refs = 3;
}

/** A record containing the waiting time.
  time: Time of event used at the reid moment.
  key: Serialized event reference that were used.
 */
message WaitingTimeRecord 
{
  message SubjectRef
  {
    string stream_id = 1; ///< References input stream with the help of the service config.
    string key = 2; ///< Corresponding message key in the input stream.
  }
  int32 waiting_time_ms = 1; ///< waiting time in milliseconds
  SubjectRef enter = 2;
  SubjectRef exit = 3;
  float score = 4;
}
